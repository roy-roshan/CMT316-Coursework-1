{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e76fa8-1a9e-4e04-8fed-a1a043859abb",
   "metadata": {},
   "source": [
    "## Cell 1 Import Libraries\n",
    "### To setup up for our text classification project, I import essential libraries and downloaded additional resources to help for data handling, preprocessing, model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "065a6112-e569-497b-8a3a-9f5bb22bda4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rosha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rosha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rosha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b953a88-eec6-46ae-87d8-651b04a894ee",
   "metadata": {},
   "source": [
    "## Cell 2: Load and Preprocess News Data\n",
    "\n",
    "### Here we are handling the loading and preprocessing of the news data. The dataset is organised into categories, and we iterate through each category to read text files, concatenate their content, and create a DataFrame. Text preprocessing functions, including lowercasing, punctuation removal, tokenization, stopword removal, and lemmatization, are applied to clean the textual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3284e6c-d4df-4080-aa35-9ab8451e4c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"datasets_coursework1/bbc\"\n",
    "os.chdir(data_dir)\n",
    "\n",
    "categories = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]\n",
    "\n",
    "news_data_list = []\n",
    "\n",
    "for category in categories:\n",
    "    files_in_category = os.listdir(category)\n",
    "    for text_file in files_in_category:\n",
    "        file_path = os.path.join(category, text_file)\n",
    "        with open(file_path, encoding='unicode_escape') as file:\n",
    "            content = ' '.join(file.readlines())\n",
    "        news_data_list.append({'news': content, 'category': category})\n",
    "\n",
    "df = pd.DataFrame(news_data_list)\n",
    "\n",
    "def preprocess_text(raw_text):\n",
    "    processed_text = raw_text.lower()\n",
    "    processed_text = ''.join([char for char in processed_text if char not in string.punctuation])\n",
    "    words = word_tokenize(processed_text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    processed_text = ' '.join(words)\n",
    "    return processed_text\n",
    "\n",
    "df['news'] = df['news'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb1df08-42d0-4de6-8ebf-cee8c4041d35",
   "metadata": {},
   "source": [
    "## Cell 3: Train-Val-Test Split\n",
    "\n",
    "#### The dataset is split into training, validation, and test sets. The variables are assigned to store the news content and corresponding categories for each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae4a8649-bcc5-4542-b557-ce0efffe1c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.20, random_state=22, stratify=df[\"category\"])\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=22, stratify=train_df[\"category\"])\n",
    "\n",
    "train_n, train_cat = train_df[\"news\"], train_df[\"category\"]\n",
    "val_n, val_cat = val_df[\"news\"], val_df[\"category\"]\n",
    "test_n, test_cat = test_df[\"news\"], test_df[\"category\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b60fcf-9c14-4a0e-97f8-8d3524cbf193",
   "metadata": {},
   "source": [
    "## Cell 4: Feature Extraction and Classification Functions\n",
    "\n",
    "### Handling both feature extraction and classification tasks. This function takes parameters: feature type, data, and categories, and dynamically adapts its operations based on the specified feature type. The features are selected, scaled, and used to train various classifiers. The best-performing classifier on the validation set is then chosen for final evaluation on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a608865c-3b81-4d78-88ef-22a7110c9b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_and_classify(feature_type, train_data, val_data, test_data, train_category, val_category, test_category):\n",
    "    # Initialise variables\n",
    "    train_feat, val_feat, test_feat = None, None, None\n",
    "\n",
    "    # Feature extraction based on feature_type\n",
    "    if feature_type == \"absolute_word_freq\":\n",
    "        vectorizer = CountVectorizer(max_features=3000)\n",
    "        train_feat = vectorizer.fit_transform(train_data).toarray()\n",
    "        val_feat = vectorizer.transform(val_data).toarray()\n",
    "        test_feat = vectorizer.transform(test_data).toarray()\n",
    "    elif feature_type == \"ngram\":\n",
    "        vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=3000)\n",
    "        train_feat = vectorizer.fit_transform(train_data).toarray()\n",
    "        val_feat = vectorizer.transform(val_data).toarray()\n",
    "        test_feat = vectorizer.transform(test_data).toarray()\n",
    "    elif feature_type == \"tfidf\":\n",
    "        vectorizer = TfidfVectorizer(max_features=3000)\n",
    "        train_feat = vectorizer.fit_transform(train_data).toarray()\n",
    "        val_feat = vectorizer.transform(val_data).toarray()\n",
    "        test_feat = vectorizer.transform(test_data).toarray()\n",
    "    elif feature_type == \"combined\":\n",
    "        vectorizer_absolute_word_freq = CountVectorizer(max_features=3000)\n",
    "        vectorizer_ngram = CountVectorizer(ngram_range=(1, 2), max_features=3000)\n",
    "        vectorizer_tfidf = TfidfVectorizer(max_features=3000)\n",
    "\n",
    "        train_feat_absolute_word_freq = vectorizer_absolute_word_freq.fit_transform(train_data).toarray()\n",
    "        val_feat_absolute_word_freq = vectorizer_absolute_word_freq.transform(val_data).toarray()\n",
    "        test_feat_absolute_word_freq = vectorizer_absolute_word_freq.transform(test_data).toarray()\n",
    "\n",
    "        train_feat_ngram = vectorizer_ngram.fit_transform(train_data).toarray()\n",
    "        val_feat_ngram = vectorizer_ngram.transform(val_data).toarray()\n",
    "        test_feat_ngram = vectorizer_ngram.transform(test_data).toarray()\n",
    "\n",
    "        train_feat_tfidf = vectorizer_tfidf.fit_transform(train_data).toarray()\n",
    "        val_feat_tfidf = vectorizer_tfidf.transform(val_data).toarray()\n",
    "        test_feat_tfidf = vectorizer_tfidf.transform(test_data).toarray()\n",
    "\n",
    "        train_feat = np.concatenate((train_feat_absolute_word_freq, train_feat_ngram, train_feat_tfidf), axis=1)\n",
    "        val_feat = np.concatenate((val_feat_absolute_word_freq, val_feat_ngram, val_feat_tfidf), axis=1)\n",
    "        test_feat = np.concatenate((test_feat_absolute_word_freq, test_feat_ngram, test_feat_tfidf), axis=1)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid feature_type. Supported values are: 'absolute_word_freq', 'ngram', 'tfidf', 'combined'.\")\n",
    "\n",
    "    # Feature selection\n",
    "    k_best_selector = SelectKBest(chi2, k=200)\n",
    "    train_feat = k_best_selector.fit_transform(train_feat, train_category)\n",
    "    val_feat = k_best_selector.transform(val_feat)\n",
    "    test_feat = k_best_selector.transform(test_feat)\n",
    "\n",
    "    # Scale features if needed\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "    train_feat = scaler.fit_transform(train_feat)\n",
    "    val_feat = scaler.transform(val_feat)\n",
    "    test_feat = scaler.transform(test_feat)\n",
    "\n",
    "    # Initialise classifiers\n",
    "    svm_clf = SVC(C=0.1, kernel='rbf')\n",
    "    rf_clf = RandomForestClassifier(random_state=42)\n",
    "    nb_clf = MultinomialNB()\n",
    "    lr_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "    classifiers = {'SVM': svm_clf, 'Random Forest': rf_clf, 'Naive Bayes': nb_clf, 'Logistic Regression': lr_clf}\n",
    "\n",
    "    # Train and evaluate each classifier\n",
    "    for name, clf in classifiers.items():\n",
    "        clf.fit(train_feat, train_category)\n",
    "        val_preds = clf.predict(val_feat)\n",
    "        accuracy = accuracy_score(val_category, val_preds)\n",
    "        conf_matrix = confusion_matrix(val_category, val_preds)\n",
    "        classification_rep = classification_report(val_category, val_preds)\n",
    "        print(f\"\\nEvaluation for {name} ({feature_type}):\")\n",
    "        print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "    # Select the best classifier based on validation performance\n",
    "    best_clf_name = max(classifiers, key=lambda x: accuracy_score(val_category, classifiers[x].predict(val_feat)))\n",
    "    best_clf = classifiers[best_clf_name]\n",
    "    print(f\"\\nBest Classifier based on Validation Accuracy ({feature_type}): {best_clf_name}\")\n",
    "\n",
    "    # Predictions on the test set with the best classifier\n",
    "    test_preds = best_clf.predict(test_feat)\n",
    "\n",
    "    # Evaluate the best classifier on the test set\n",
    "    test_accuracy = accuracy_score(test_category, test_preds)\n",
    "    test_classification_rep = classification_report(test_category, test_preds, output_dict=True)\n",
    "\n",
    "    # Print the evaluation metrics for the test set with the best classifier\n",
    "    print(f\"\\nTest Set Evaluation with the Best Classifier ({feature_type}): {best_clf_name}\")\n",
    "    print(\"Test Accuracy: {:.2f}%\".format(test_accuracy * 100))\n",
    "\n",
    "    # Print macro precision, recall, and F1-score\n",
    "    macro_precision = test_classification_rep['macro avg']['precision']\n",
    "    macro_recall = test_classification_rep['macro avg']['recall']\n",
    "    macro_f1_score = test_classification_rep['macro avg']['f1-score']\n",
    "\n",
    "    print(\"Macro Precision: {:.2f}\".format(macro_precision * 100))\n",
    "    print(\"Macro Recall: {:.2f}\".format(macro_recall * 100))\n",
    "    print(\"Macro F1-score: {:.2f}\".format(macro_f1_score * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e16da0-b96e-4949-936f-52a3a57e39d3",
   "metadata": {},
   "source": [
    "### Cell 5: Model Training and Evaluation\n",
    "\n",
    "#### Here the function is called with the different choice of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e4c0d23-775d-47c5-a3f6-d6744289c0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation for SVM (absolute_word_freq):\n",
      "Accuracy: 86.80%\n",
      "\n",
      "Evaluation for Random Forest (absolute_word_freq):\n",
      "Accuracy: 95.22%\n",
      "\n",
      "Evaluation for Naive Bayes (absolute_word_freq):\n",
      "Accuracy: 96.07%\n",
      "\n",
      "Evaluation for Logistic Regression (absolute_word_freq):\n",
      "Accuracy: 94.94%\n",
      "\n",
      "Best Classifier based on Validation Accuracy (absolute_word_freq): Naive Bayes\n",
      "\n",
      "Test Set Evaluation with the Best Classifier (absolute_word_freq): Naive Bayes\n",
      "Test Accuracy: 95.28%\n",
      "Macro Precision: 95.26\n",
      "Macro Recall: 95.29\n",
      "Macro F1-score: 95.25\n",
      "\n",
      "Evaluation for SVM (ngram):\n",
      "Accuracy: 85.67%\n",
      "\n",
      "Evaluation for Random Forest (ngram):\n",
      "Accuracy: 94.94%\n",
      "\n",
      "Evaluation for Naive Bayes (ngram):\n",
      "Accuracy: 95.79%\n",
      "\n",
      "Evaluation for Logistic Regression (ngram):\n",
      "Accuracy: 95.22%\n",
      "\n",
      "Best Classifier based on Validation Accuracy (ngram): Naive Bayes\n",
      "\n",
      "Test Set Evaluation with the Best Classifier (ngram): Naive Bayes\n",
      "Test Accuracy: 96.18%\n",
      "Macro Precision: 96.13\n",
      "Macro Recall: 96.16\n",
      "Macro F1-score: 96.12\n",
      "\n",
      "Evaluation for SVM (tfidf):\n",
      "Accuracy: 87.92%\n",
      "\n",
      "Evaluation for Random Forest (tfidf):\n",
      "Accuracy: 94.66%\n",
      "\n",
      "Evaluation for Naive Bayes (tfidf):\n",
      "Accuracy: 95.79%\n",
      "\n",
      "Evaluation for Logistic Regression (tfidf):\n",
      "Accuracy: 94.94%\n",
      "\n",
      "Best Classifier based on Validation Accuracy (tfidf): Naive Bayes\n",
      "\n",
      "Test Set Evaluation with the Best Classifier (tfidf): Naive Bayes\n",
      "Test Accuracy: 94.83%\n",
      "Macro Precision: 94.83\n",
      "Macro Recall: 94.79\n",
      "Macro F1-score: 94.78\n",
      "\n",
      "Evaluation for SVM (combined):\n",
      "Accuracy: 85.39%\n",
      "\n",
      "Evaluation for Random Forest (combined):\n",
      "Accuracy: 94.66%\n",
      "\n",
      "Evaluation for Naive Bayes (combined):\n",
      "Accuracy: 94.66%\n",
      "\n",
      "Evaluation for Logistic Regression (combined):\n",
      "Accuracy: 93.54%\n",
      "\n",
      "Best Classifier based on Validation Accuracy (combined): Random Forest\n",
      "\n",
      "Test Set Evaluation with the Best Classifier (combined): Random Forest\n",
      "Test Accuracy: 95.28%\n",
      "Macro Precision: 95.89\n",
      "Macro Recall: 95.05\n",
      "Macro F1-score: 95.39\n"
     ]
    }
   ],
   "source": [
    "feature_and_classify(\"absolute_word_freq\", train_n, val_n, test_n, train_cat, val_cat, test_cat)\n",
    "feature_and_classify(\"ngram\", train_n, val_n, test_n, train_cat, val_cat, test_cat)\n",
    "feature_and_classify(\"tfidf\", train_n, val_n, test_n, train_cat, val_cat, test_cat)\n",
    "feature_and_classify(\"combined\", train_n, val_n, test_n, train_cat, val_cat, test_cat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
